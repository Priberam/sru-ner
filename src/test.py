"""
This file is used to test a trained model.
Run it by doing 

python src/test.py --cfg_path {cfg_path}

where {cfg_path} is the path to the resolved config that was generated by Hydra
during training. It automatically uses a checkpoint with 'F1-average' in its name.
"""

import autorootcwd
import lightning.pytorch
import lightning.pytorch.loggers
from omegaconf import DictConfig
from lightning import seed_everything
from src.utils import logger
from typing import Tuple, Optional
from src.modules.ner_module import LitNERmodel
from src.modules.data_module import NERDataModule
import os
import lightning
import argparse
from omegaconf import OmegaConf
import torch
import pickle

log = logger.get_pylogger("NER")
os.environ["TOKENIZERS_PARALLELISM"] = "false"
torch.set_float32_matmul_precision("high")

PRINT_TO_TEXT = True # Saves predictions to a txt file near checkpoints folder


def test(cfg: DictConfig, additional_trainer_args: dict = {}) -> Tuple[dict, dict]:
    """Tests a model from a checkpoint.

    Args:
        cfg (DictConfig): Configuration composed by Hydra.

    Returns:
        Tuple[dict, dict]: Dict with metrics and dict with all instantiated
        objects.
    """
    # set seed for random number generators in pytorch, numpy and python.random
    if cfg.get("seed"):
        log.info(f"Seed everything with <{cfg.seed}>")
        seed_everything(cfg.seed, workers=True)
    else:
        log.info("Skipping seeding.")

    # Get the checkpoint info
    check_path = None
    if "saved_ckpt_paths" in cfg:
        # Look for F1-average checkpoint
        for k, v in cfg.saved_ckpt_paths.items():
            if "F1-average" in k:
                check_path = v

        if not check_path:
            log.error("Checkpoint not found!")
        else:
            log.info(f"Loading checkpoint at {check_path}")
    else:
        raise ValueError("Checkpoint not found!")

    # Init lightning datamodule
    log.info("Instantiating datamodule from checkpoint...")
    datamodule = NERDataModule.load_from_checkpoint(check_path, input_data=cfg.datasets)

    # Manually call the datamodule setup in order to have the actions vocab
    datamodule.setup(stage="test")

    # Init lightning model
    log.info("Instantiating lightning model from checkpoint...")
    overwrite_args = {"actions_vocab": datamodule.actions_vocab}
    model = LitNERmodel.load_from_checkpoint(check_path, **overwrite_args)

    # Init lightning trainer
    log.info("Instantiating trainer")
    trainer = lightning.Trainer(
        default_root_dir=cfg.checkpoint_dir,
        max_epochs=cfg.train.num_epochs,
        devices=1,
        accelerator="gpu",
        enable_progress_bar=True,
        enable_model_summary=False,
        deterministic=cfg.train.use_deterministic,
        enable_checkpointing=False,
        logger=False,
        limit_test_batches=(
            cfg.train.limit_test_batches if "limit_test_batches" in cfg.train else 1.0
        ),
        **additional_trainer_args,
    )

    trainer.test(model=model, datamodule=datamodule)

    output = {"data": datamodule.test_dataset.data, "pred_df": model.ner_metrics.data}
    if hasattr(model, "joined_ner_metrics"):
        output["joined_pred_df"] = model.joined_ner_metrics.data

    if PRINT_TO_TEXT:
        # Gather data in a dictionary
        len_data = output["joined_pred_df"]["sent_id"].max()
        output_dict = dict()
        for sent_id in range(len_data):
            output_dict[sent_id] = {
                "raw_sentence": output["data"]["raw_sentences"][sent_id],
                "wordpieces": output["data"]["wordpieces"][sent_id],
                "tokens": output["data"]["tokens"][sent_id],
                "pred_spans": [],
                "gold_spans": [],
            }

            this_sent_spans = output["joined_pred_df"][
                output["joined_pred_df"]["sent_id"] == sent_id
            ]

            for _, row in this_sent_spans.iterrows():
                start_w = row["start_idx"]
                end_w = row["end_idx"]
                start_char = output["data"]["offset_mapping_sentence"][sent_id][
                    start_w
                ][0][0]
                end_char = output["data"]["offset_mapping_sentence"][sent_id][
                    end_w - 1
                ][-1][-1]
                text = output["data"]["raw_sentences"][sent_id][start_char:end_char]

                if row["gold_tag"]:
                    for tag in row["gold_tag"]:
                        output_dict[sent_id]["gold_spans"].append(
                            (start_char, end_char, tag, text)
                        )

                if row["pred_tag"]:
                    for tag in row["pred_tag"]:
                        output_dict[sent_id]["pred_spans"].append(
                            (start_char, end_char, tag, text)
                        )

            output_dict[sent_id]["gold_spans"] = sorted(
                output_dict[sent_id]["gold_spans"], key=lambda x: (x[0], -x[1], x[2])
            )
            output_dict[sent_id]["pred_spans"] = sorted(
                output_dict[sent_id]["pred_spans"], key=lambda x: (x[0], -x[1], x[2])
            )

        # Save in txt
        parent_dir = os.path.dirname(cfg.checkpoint_dir)
        file_save = "_".join([cfg.experiment_name, cfg.task_name, "test_results.txt"])
        file_save = os.path.join(parent_dir, file_save)
        with open(file_save, "w") as f:
            for sent_id, data in output_dict.items():
                f.write(f"sent_id={sent_id}\n")
                f.write("Raw sentence:\n")
                f.write(data["raw_sentence"])
                f.write("\nWordpieces:\n")
                f.write(str(data["wordpieces"]))
                f.write("\nTokens:\n")
                f.write(str(data["tokens"]))
                f.write("\nPred:\n")
                f.write(str(data["pred_spans"]))
                f.write("\n-----------------\n")

    # Save at the same level as checkpoints directory
    parent_dir = os.path.dirname(cfg.checkpoint_dir)
    output_container_path = os.path.join(parent_dir, "test_results.pkl")
    with open(output_container_path, "wb") as f:
        pickle.dump(output, f)

    return output


def main(args) -> None:
    config = OmegaConf.load(args.cfg_path)
    test(cfg=config)


if __name__ == "__main__":

    parser = argparse.ArgumentParser(description="Test the NER model.")
    parser.add_argument(
        "--cfg_path",
        type=str,
        required=True,
        help="Path to the resolved config, generated by Hydra during training.",
    )

    args = parser.parse_args()
    main(args)
